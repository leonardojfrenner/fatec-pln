# Chat API com LLM

API REST para interagir com modelo de linguagem Qwen 3 usando FastAPI.

## Estrutura do Projeto

```
chat/
‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îî‚îÄ‚îÄ app.py          # Rotas da API FastAPI
‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îî‚îÄ‚îÄ llm.py          # Servi√ßo do modelo LLM
‚îú‚îÄ‚îÄ run_api.py          # Script para iniciar a API
‚îú‚îÄ‚îÄ test_api.py         # Script para testar a API
‚îî‚îÄ‚îÄ README.md           # Este arquivo
```

## Como Executar

### üê≥ Op√ß√£o 1: Com Docker (Recomendado)

**Simples e sem configura√ß√£o:**

```powershell
# Construir e iniciar
docker-compose up -d

# Ver logs
docker-compose logs -f
```

A API estar√° dispon√≠vel em: `http://localhost:8000`

üìñ **Documenta√ß√£o completa:** [DOCKER.md](DOCKER.md)

### üêç Op√ß√£o 2: Direto com Python

**1. Ativar o ambiente virtual:**
```powershell
.\venv\Scripts\activate
```

**2. Instalar depend√™ncias:**
```powershell
pip install -r requirements.txt
```

**3. Iniciar a API:**
```powershell
python run_api.py
```

A API estar√° dispon√≠vel em: `http://localhost:8000`

### üìö Acessar a documenta√ß√£o

Abra no navegador:
- **Documenta√ß√£o interativa (Swagger)**: http://localhost:8000/docs
- **Documenta√ß√£o alternativa (ReDoc)**: http://localhost:8000/redoc

## Endpoints da API

### GET `/`
Informa√ß√µes b√°sicas da API

**Exemplo:**
```bash
curl http://localhost:8000/
```

### GET `/saude`
Verifica o status da API e do modelo

**Exemplo:**
```bash
curl http://localhost:8000/saude
```

**Response:**
```json
{
  "status": "saudavel",
  "modelo_carregado": true,
  "nome_modelo": "Qwen/Qwen3-0.6B"
}
```

### POST `/pergunta` üí¨ (modo s√≠ncrono)

**Request Body:**
```json
{
  "question": "Quem foi a primeira pessoa no espa√ßo?",
  "max_tokens": 256
}
```

**Response:**
```json
{
  "question": "Quem foi a primeira pessoa no espa√ßo?",
  "thinking": "Analisando a hist√≥ria da explora√ß√£o espacial...",
  "response": "Yuri Gagarin foi a primeira pessoa no espa√ßo..."
}
```

### POST `/pergunta-stream` ‚ö° (modo streaming)

**Request Body:** (igual ao s√≠ncrono)
```json
{
  "question": "Quem foi a primeira pessoa no espa√ßo?",
  "max_tokens": 512
}
```

**Response:** Server-Sent Events (SSE)
```
data: {'type': 'thinking_chunk', 'content': 'Analisando'}
data: {'type': 'thinking_chunk', 'content': ' a'}
data: {'type': 'thinking_chunk', 'content': ' hist√≥ria...'}
data: {'type': 'thinking', 'content': 'Analisando a hist√≥ria...'}
data: {'type': 'response_chunk', 'content': 'Yuri'}
data: {'type': 'response_chunk', 'content': ' Gagarin'}
data: {'type': 'response_chunk', 'content': '...'}
data: {'type': 'done'}
```

**Tipos de eventos:**
- `thinking_chunk`: Peda√ßos do pensamento em tempo real
- `thinking`: Pensamento completo
- `response_chunk`: Peda√ßos da resposta em tempo real
- `response`: Resposta completa (opcional)
- `done`: Streaming finalizado

**Exemplo com curl:**
```bash
curl -X POST http://localhost:8000/pergunta \
  -H "Content-Type: application/json" \
  -d "{\"question\": \"Quem foi a primeira pessoa no espa√ßo?\"}"
```

**Exemplo com PowerShell:**
```powershell
$body = @{
    question = "Quem foi a primeira pessoa no espa√ßo?"
    max_tokens = 512
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:8000/pergunta" -Method Post -Body $body -ContentType "application/json"
```

**Exemplo com Python:**
```python
import requests

response = requests.post(
    "http://localhost:8000/pergunta",
    json={
        "question": "Quem foi a primeira pessoa no espa√ßo?",
        "max_tokens": 512
    }
)

result = response.json()
print("üß† Pensamento:", result["thinking"])
print("üí¨ Resposta:", result["response"])
```

### GET `/modelo`
Retorna informa√ß√µes sobre o modelo carregado

**Exemplo:**
```bash
curl http://localhost:8000/modelo
```

**Response:**
```json
{
  "nome_modelo": "Qwen/Qwen3-0.6B",
  "dispositivo": "cpu",
  "tipo_modelo": "Qwen2ForCausalLM"
}
```

## Tecnologias

- **FastAPI**: Framework web moderno e r√°pido
- **Transformers**: Biblioteca da Hugging Face para modelos de linguagem
- **Qwen 3 0.6B**: Modelo de linguagem compacto com modo de pensamento
- **Uvicorn**: Servidor ASGI de alta performance

## Recursos

- ‚úÖ Modelo muito leve (~600MB) que roda em CPU
- ‚úÖ Modo "thinking" mostra o racioc√≠nio do modelo
- ‚úÖ Documenta√ß√£o autom√°tica com Swagger
- ‚úÖ Valida√ß√£o de dados com Pydantic
- ‚úÖ API REST simples e r√°pida

## Notas

- O modelo √© carregado na inicializa√ß√£o da API (pode demorar alguns segundos)
- O modelo usa CPU (pode levar alguns segundos para perguntas complexas)
- O modo "thinking" mostra o racioc√≠nio interno do modelo antes da resposta

## üìä Sobre o Modelo Qwen3-0.6B

O **Qwen/Qwen3-0.6B** √© um modelo muito compacto de 600MB:

**Vantagens:**
- ‚úÖ Muito r√°pido - gera respostas em segundos
- ‚úÖ Roda em CPU com pouca RAM (4GB suficiente)
- ‚úÖ Pequeno e f√°cil de baixar
- ‚úÖ Tem modo "thinking" integrado

**Limita√ß√µes:**
- ‚ö†Ô∏è Conhecimento limitado (modelo muito pequeno)
- ‚ö†Ô∏è Pode dar respostas imprecisas ou confusas em portugu√™s
- ‚ö†Ô∏è Melhor em ingl√™s que em portugu√™s
- ‚ö†Ô∏è N√£o √© adequado para perguntas complexas

**Para melhores resultados:**

1. **Fa√ßa perguntas simples e diretas**
   - ‚úÖ "Quem foi Yuri Gagarin?"
   - ‚ùå "Explique detalhadamente a hist√≥ria da explora√ß√£o espacial"

2. **Use perguntas em ingl√™s quando poss√≠vel**
   - O modelo foi treinado principalmente em ingl√™s

3. **Considere usar modelos maiores:**
   - Qwen/Qwen2.5-7B (melhor qualidade, precisa de mais RAM)
   - Modelos da fam√≠lia Llama
   - APIs comerciais (OpenAI, Anthropic, Groq)

### Como trocar o modelo

Para usar um modelo diferente, edite `service/llm.py`:

```python
# Linha 9 - Trocar o modelo
def __init__(self, model_name: str = "Qwen/Qwen3-0.6B"):  # ‚Üê Atual (0.6B)
    
# Op√ß√µes de modelos:
# Modelo pequeno (mais r√°pido, menos preciso) - ATUAL
def __init__(self, model_name: str = "Qwen/Qwen3-0.6B"):  # 0.6B - muito r√°pido ‚úÖ
# Modelo m√©dio (balanceado, mais preciso)
def __init__(self, model_name: str = "microsoft/phi-2"):  # 2.7B - melhor qualidade
# Modelo m√©dio-grande
def __init__(self, model_name: str = "Qwen/Qwen2.5-1.5B"):  # 1.5B
# Modelo grande (melhor qualidade, precisa GPU ou muita RAM)
def __init__(self, model_name: str = "Qwen/Qwen2.5-7B"):  # 7B - melhor qualidade
```

**Nota:** Modelos maiores precisam de mais RAM e s√£o mais lentos na CPU.

**Recomenda√ß√µes por RAM:**
- 4-8GB RAM: `Qwen/Qwen3-0.6B` (atual ‚úÖ - mais r√°pido)
- 8-16GB RAM: `microsoft/phi-2` (mais preciso)
- 16GB+ RAM: `Qwen/Qwen2.5-7B` (melhor qualidade)
